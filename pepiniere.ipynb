{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pépinière\n",
    "This notebook contains a bunch of scripts that can process and clean up data from GISAID/BVBRC. It takes any number of fasta files and combines them into one final file. It requires metadata with time, species and continent of isolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the fasta files and metadata files here and specify which website they were downloaded from. The sequences from the different files will be concatenated into a single file and duplicates will be removed during the process.\n",
    "\n",
    "GISAID files should be dowloaded with fasta header format: Type|  DNA INSDC   | Isolate name   | DNA Accession no., YYYY-MM-DD format and both space replacement/removal checked.<p>\n",
    "BVBRC files should be downloaded in standard format and will be reformatted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple of files containing the sequences\n",
    "# (<path>, <GISAID/BVBRC>)\n",
    "seq_files=(('/Users/threecats/Data/FluSeq/trees/h5_all/raw/gisaid_all-h5-170101.fasta', 'GISAID'),\n",
    "           ('/Users/threecats/Data/FluSeq/trees/h5_all/raw/gisaid_all-h5-170101-230209.fasta', 'GISAID'),\n",
    "           ('/Users/threecats/Data/FluSeq/trees/h5_all/raw/BVBRC_all-h5-230214-1.fasta', 'BVBRC'),\n",
    "           ('/Users/threecats/Data/FluSeq/trees/h5_all/raw/BVBRC_all-h5-230214-2.fasta', 'BVBRC'))\n",
    "\n",
    "# tuple of files containing metadata\n",
    "# (<path>, <GISAID/BVBRC>)\n",
    "metadata_files=(('/Users/threecats/Data/FluSeq/trees/h5_all/raw/gisaid_mdata_all-h5-170101.tsv', 'GISAID'),\n",
    "                ('/Users/threecats/Data/FluSeq/trees/h5_all/raw/gisaid_mdata_all-h5-170101-230214.tsv', 'GISAID'),\n",
    "                ('/Users/threecats/Data/FluSeq/trees/h5_all/raw/BVBRC_mdata_all-h5-230214-1.txt', 'BVBRC'),\n",
    "                ('/Users/threecats/Data/FluSeq/trees/h5_all/raw/BVBRC_mdata_all-h5-230214-2.txt', 'BVBRC'))\n",
    "\n",
    "# path the species list if used\n",
    "species_list='/Users/threecats/Data/git/pepiniere/species_list_sorted.txt'\n",
    "\n",
    "# string pointing at the directory where you want the data\n",
    "output_dir='/Users/threecats/Data/FluSeq/trees/h5_all'\n",
    "\n",
    "# name of final dataset\n",
    "name='allH5'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set up\n",
    "Import packages, set up functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under what length should we look for a second ORF\n",
    "min_orf_len=1000 # (nts)\n",
    "# what is the lowest ORF length acceptable\n",
    "orf_cutoff=0.9 # (relative to average)\n",
    "\n",
    "# should we output report files?\n",
    "report_files = True\n",
    "\n",
    "# should we try to correct common metadata errors?\n",
    "corr_mdata = True\n",
    "\n",
    "# should we throw out all files without minimal metadata (ie date)\n",
    "purge_nomdata = True\n",
    "\n",
    "# should we only keep on copy of all unique sequences?\n",
    "purge_unique = True\n",
    "\n",
    "# should we remove ORFs without stop codon?\n",
    "purge_nostop = True\n",
    "\n",
    "# sequences with proportion of undeterminate characters over the threshold will be removed\n",
    "# set to 1 to not remove sequences with indetermined positions\n",
    "max_indeterminate=1\n",
    "\n",
    "# make a file with all short/nostop orfs and the initial sequence?\n",
    "write_badorf=False\n",
    "\n",
    "continents=('North_America','South_America','Europe','Oceania','Africa','Asia','Antarctica')\n",
    "\n",
    "seq_counts={}\n",
    "for folder in ('temp','data','tree','metadata','annotations'):\n",
    "    if not os.path.exists(f'{output_dir}/{folder}/'):\n",
    "        os.mkdir(f'{output_dir}/{folder}/')\n",
    "if report_files==True:\n",
    "    if not os.path.exists(f'{output_dir}/reports/'):\n",
    "        os.mkdir(f'{output_dir}/reports/')\n",
    "\n",
    "version='23-02-14'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function removes illegal characters from strain names and metadata\n",
    "# also strips whitespaces from front and back\n",
    "def remove_illegal_chars(id):\n",
    "    id=id.strip(' ')\n",
    "    id = '_'.join(id.split(' '))\n",
    "    # there probs is a more elegant way to do this, but its fast enough right now\n",
    "    for char in ('(',')',';',':','.',\",\",\"'\",'?','（','）',' '):\n",
    "        id = '-'.join(id.split(char))\n",
    "    return id\n",
    "\n",
    "# this function fixes common errors in metadata and returns a formatted string\n",
    "# mdata should be list date  host    host group    continent   country passage history\n",
    "# species_dict should be dict species:group\n",
    "def correct_mdata(strain, mdata):\n",
    "    report_data=[]\n",
    "    fixed_date, fixed_continent = '',''\n",
    "    date_cutoff=2 # decade cut-off for 19XX or 20XX date guessing\n",
    "\n",
    "    # fixing date\n",
    "    date_parts = fix_date(mdata[0]).split('-')\n",
    "    if date_parts[1] != 'XX' and int(date_parts[1]) > 12:\n",
    "        report_data.append[f'{strain}: fixed date: \"{mdata[0]}\" -> \"{date_parts[0]}-{date_parts[2]}-{date_parts[1]}\"']\n",
    "        fixed_date =f'{date_parts[0]}-{date_parts[2]}-{date_parts[1]}'\n",
    "    else:\n",
    "        fixed_date=fix_date(mdata[0])\n",
    "    if fixed_date=='XXXX-XX-XX':\n",
    "    # no date provided, or date grivesouly misformatted\n",
    "    # try to find date in strain name, else just keep as missing and output warning\n",
    "        strain_date4, strain_date2=strain.rstrip('/').split('/')[-1][:4], strain.strip('/').split('/')[-1][:2]\n",
    "        if len(strain_date4)==4 and strain_date4.isnumeric():\n",
    "        # 4 digit number at end of strain name\n",
    "            fixed_date=strain_date4+'-XX-XX'\n",
    "        elif strain_date2.isnumeric():\n",
    "        # no 4 digit, but 2 digit number at end of strain name\n",
    "            if int(strain_date2[0])<=date_cutoff:\n",
    "                fixed_date=f'20{strain_date2}-XX-XX'\n",
    "            else:\n",
    "                fixed_date=f'19{strain_date2}-XX-XX'\n",
    "\n",
    "        if fixed_date != 'XXXX-XX-XX':\n",
    "            report_data.append(f'{strain}: date \"{mdata[0]}\" could not be parsed, guessed as \"{fixed_date}\"')\n",
    "        else:\n",
    "            print(f'warning, no date for {strain}')\n",
    "            report_data.append(f'WARNING no date for {strain}')\n",
    "    # fixing geography\n",
    "    if remove_illegal_chars(mdata[3]) not in continents and remove_illegal_chars(mdata[4]) in continents:\n",
    "        fixed_continent =f'{remove_illegal_chars(mdata[4])}\\t{remove_illegal_chars(mdata[3])}'\n",
    "        report_data.append(f'{strain}: fixed continent: \"{mdata[3]}/{mdata[4]}\" -> \"{mdata[4]}/{mdata[3]}\"')\n",
    "    elif remove_illegal_chars(mdata[3]) not in continents and remove_illegal_chars(mdata[4]) not in continents:\n",
    "        fixed_continent = f'unknown\\t{remove_illegal_chars(mdata[4])}' \n",
    "        report_data.append(f'{strain}: fixed continent: \"{mdata[3]}/{mdata[4]}\" -> \"unknown/{mdata[4]}\"')\n",
    "    else:\n",
    "        fixed_continent = f'{remove_illegal_chars(mdata[3])}\\t{remove_illegal_chars(mdata[4])}'\n",
    "    return(f'{fixed_date}\\t{mdata[1]}\\t{mdata[2]}\\t{fixed_continent}\\t{mdata[5]}', report_data)\n",
    "\n",
    "def fix_date(date):\n",
    "    d=date.split('-')\n",
    "    if ''.join(d).isnumeric():\n",
    "    # only numbers, good sign\n",
    "        if len(date)<10:\n",
    "        # lower precision date\n",
    "            if len(d[0])!=4:\n",
    "            # MM-YYYY format probably??? turn around\n",
    "                date=f\"{d[1]}-{d[0]}\"\n",
    "\n",
    "        elif len(d[2])==4:\n",
    "        # date in DD-MM-YYYY\n",
    "            date=f\"{d[2]}-{d[1]}-{d[0]}\"   \n",
    "    else:\n",
    "    # letters in date, uh oh. for now just ditch anything but year\n",
    "        for part in d:\n",
    "            if len(part)==4 and part.isnumeric():\n",
    "            # this one should be the year, else would not have 4 digits\n",
    "                date=part+'-XX-XX'\n",
    "                break\n",
    "        else:\n",
    "        # can't find a 4 digit year, either missing date or grievously misformatted\n",
    "            date='XXXX-XX-XX'\n",
    "\n",
    "    # everything should be numbers or XXXX by now, just pad it all in case it's needed\n",
    "    while len(date)<10:\n",
    "        date+='-XX'\n",
    "\n",
    "    return date\n",
    "\n",
    "def inderterminate_ratio(seq):\n",
    "    return(sum([0 if c in ('A','T','C','G') else 1 for c in seq])/len(seq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merging files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming\n",
    "Standardizing fasta id format into a GISAID-like \\<subtype\\>|\\<accession number\\>|\\<strain name\\>. For GISAID, sequences without accession number will have the EPI number instead\n",
    "\n",
    "Also concatenates the sequences into a single temp file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize counter\n",
    "seq_counts['total']=0\n",
    "\n",
    "with open(f'{output_dir}/temp/all.fasta','w') as fout:\n",
    "    for file in seq_files:\n",
    "        \n",
    "        if file[1]=='GISAID':\n",
    "            for record in SeqIO.parse(file[0], 'fasta'):\n",
    "                rec_split=record.id.split('|')\n",
    "                if rec_split[1]!='':\n",
    "                # has an accession number\n",
    "                    record_id=f'{rec_split[0][4:]}|{rec_split[1]}|{rec_split[2]}|'\n",
    "                else:\n",
    "                # has no accession number  \n",
    "                    record_id=f'{rec_split[0][4:]}|EPI{rec_split[3]}|{rec_split[2]}|'\n",
    "                fout.write(f'>{record_id}\\n{record.seq}\\n')\n",
    "                seq_counts['total']+=1\n",
    "       \n",
    "        elif file[1]=='BVBRC':\n",
    "            # easiest way to deal with these is actually reading in metadata\n",
    "            # the fasta is formatted by idiots  >:(\n",
    "            # dict is accession_number:(subtype, strain name)\n",
    "            record_ids={}\n",
    "            for metafile in metadata_files:\n",
    "                if metafile[1]=='BVBRC':\n",
    "                    with open(metafile[0], 'r') as f:\n",
    "                        f.readline()\n",
    "                        for line in f:\n",
    "                            l=line.split('\\t')\n",
    "                            record_ids[l[43][1:-1]]=f\"{l[21][1:-1]}{'Nx' if len(l[21][1:-1])==2 else ''}|{l[43][1:-1]}|{'_'.join(l[15][1:-1].split(' '))}|**gb\"\n",
    "            for record in SeqIO.parse(file[0], 'fasta'):\n",
    "                fout.write(f'>{record_ids[record.id[5:]]}\\n{record.seq}\\n')\n",
    "                seq_counts['total']+=1\n",
    "        else:\n",
    "            print(f'cannot recognize {file[1]} format for {file[0]}, has to be GISAID or BVBRC, skipping')\n",
    "print(f'combined sequence total: {seq_counts[\"total\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First de-duplication round\n",
    "Here we only remove sequences that are 100% identical and with the same accession number/strain name. This is done in 2 rounds, first looks for duplicate accession numbers, second for duplicate strain names. I am assuming that all sequences with the same accession number are identical, I don't actually check for it. For strain names it gets more complicated since the sequences might differ.\n",
    "\n",
    "BVBRC agrees with Genbank while GISAID seems to make mistakes, so the former is prioritised. Additionally if a Genbank sequence is updated, GISAID does not update the entry while BVBRC does (I think? IRD used to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize counter\n",
    "seq_counts['dedup']=0\n",
    "blacklist=[]\n",
    "db_counts={'gisaid':0,'both':0,'bvbrc':0}\n",
    "report_data={}\n",
    "\n",
    "id_dict=defaultdict(list)\n",
    "with open(f'{output_dir}/temp/all.fasta','r') as f:\n",
    "    # get accession number from fasta id, ignore fasta without id\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            an=line.split('|')[1]\n",
    "            if an != '':\n",
    "                id_dict[an].append(line.strip().split('|',1)[1])\n",
    "\n",
    "for records in id_dict.values():\n",
    "    if len(records)>1:\n",
    "        if ''.join(records).count('**gb')==1:\n",
    "        # only one seq from BVBRC, keep that one\n",
    "            k=''\n",
    "            v=[]\n",
    "            db_counts['both']+=1\n",
    "            for record in records:\n",
    "                if '**gb' not in record:\n",
    "                    blacklist.append(record)\n",
    "                    v.append(record)\n",
    "                else:\n",
    "                    k=record\n",
    "            report_data[k]=v\n",
    "\n",
    "        elif ''.join(records).count('**gb')==0:\n",
    "        # only GISAID seqs, keep first\n",
    "            blacklist+=records[1:]\n",
    "            report_data[records[0]]=records[1:]\n",
    "        \n",
    "        else:\n",
    "        # several BVBRC, keep first\n",
    "            if ''.join(records).count('**gb')!=len(records):\n",
    "            # at least one GISAID\n",
    "                db_counts['both']+=1\n",
    "            for record in records:\n",
    "                if '**gb' in record:\n",
    "                    records.remove(record)\n",
    "                    report_data[record]=records\n",
    "                    break\n",
    "            blacklist+=records\n",
    "\n",
    "# weed out first batch\n",
    "seqs_to_keep=defaultdict(list)\n",
    "with open(f'{output_dir}/temp/id_filtered.fasta', 'w') as f:\n",
    "    for record in SeqIO.parse(f'{output_dir}/temp/all.fasta', 'fasta'):\n",
    "        if record.id.split('|',1)[1] in blacklist:\n",
    "        # found a blacklisted record, remove one entry in blacklist\n",
    "        # this is important if the same seq is present several times\n",
    "        # if we don't remove we keep none of them\n",
    "            blacklist.remove(record.id.split('|',1)[1])\n",
    "        else:\n",
    "            if '**gb' not in record.id:\n",
    "                db_counts['gisaid']+=1\n",
    "            else:\n",
    "                db_counts['bvbrc']+=1\n",
    "            seq_counts['dedup']+=1\n",
    "            f.write(f'>{record.id}\\n{record.seq}\\n')\n",
    "\n",
    "if report_files == True:\n",
    "# put out file with dupe info\n",
    "    with open(f'{output_dir}/reports/deduplication.txt', 'w') as f:\n",
    "        f.write('kept\\treplaces\\n')\n",
    "        for k,v in report_data.items():\n",
    "            f.write(f'{k}\\t{\",\".join(v)}\\n')\n",
    "\n",
    "print(f'found and removed {seq_counts[\"total\"]-seq_counts[\"dedup\"]} duplicate sequences ({(seq_counts[\"total\"]-seq_counts[\"dedup\"])/seq_counts[\"total\"]*100:.2f}%)')\n",
    "print(f\"final set contains {db_counts['gisaid']+db_counts['bvbrc']} sequences,\"+\n",
    "      f\" of which {db_counts['gisaid']} from GISAID ({db_counts['gisaid']/(db_counts['gisaid']+db_counts['bvbrc'])*100:.2f}%)\"+\n",
    "      f\" and {db_counts['bvbrc']} from BVBRC ({db_counts['bvbrc']/(db_counts['bvbrc']+db_counts['gisaid'])*100:.2f}%),\"+\n",
    "      f\" {db_counts['both']} of which were also in GISAID ({db_counts['both']/db_counts['bvbrc']*100:.2f}%)\")\n",
    "if report_files==True:\n",
    "    print(f'wrote IDs of duplicates to {output_dir}/reports/deduplication.txt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fixing and formatting metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GISAID metadata is downloaded as xlsx file and has been converted to tsv. Sadly some people put characters into the metadata that break the whole process so first we have to fix it.<p>\n",
    "BVBRC metadata just gets concatenated and the quotation marks removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_dir}/metadata/GISAID_fixed.tsv', 'w') as fout:\n",
    "    # write header\n",
    "    fout.write('Isolate_Id\\tPB2 Segment_Id\\tPB1 Segment_Id\\tPA Segment_Id\\tHA Segment_Id\\tNP Segment_Id\\tNA Segment_Id\\tMP Segment_Id\\tNS Segment_Id\\tHE Segment_Id\\tP3 Segment_Id\\tIsolate_Name\\tSubtype\\tLineage\\tPassage_History\\tLocation\\tHost\\tIsolate_Submitter\\tSubmitting_Lab\\tSubmitting_Sample_Id\\tAuthors\\tPublication\\tOriginating_Lab\\tOriginating_Sample_Id\\tCollection_Date\\tNote\\tUpdate_Date\\tSubmission_Date\\tAntigen_Character\\tAnimal_Vaccin_Product\\tAdamantanes_Resistance_geno\\tOseltamivir_Resistance_geno\\tZanamivir_Resistance_geno\\tPeramivir_Resistance_geno\\tOther_Resistance_geno\\tAdamantanes_Resistance_pheno\\tOseltamivir_Resistance_pheno\\tZanamivir_Resistance_pheno\\tPeramivir_Resistance_pheno\\tOther_Resistance_pheno\\tHost_Age\\tHost_Age_Unit\\tHost_Gender\\tPatient_Status\\tZip_Code\\tOutbreak\\tPathogen_Test_Info\\tIs_Vaccinated\\tHuman_Specimen_Source\\tAnimal_Specimen_Source\\tAnimal_Health_Status\\tDomestic_Status\\tPMID\\tPB2 INSDC_Upload\\tPB1 INSDC_Upload\\tPA INSDC_Upload\\tHA INSDC_Upload\\tNP INSDC_Upload\\tNA INSDC_Upload\\tMP INSDC_Upload\\tNS INSDC_Upload\\tHE INSDC_Upload\\tP3 INSDC_Upload')\n",
    "    done={'',} # keep track of samples already added\n",
    "    for file in metadata_files:\n",
    "        if file[1]=='GISAID':\n",
    "            with open(file[0], 'r') as fin:\n",
    "                curr_line=''\n",
    "                fin.readline() # skip header\n",
    "                for line in fin:\n",
    "                    if line.startswith('EPI'):\n",
    "                        if curr_line not in done:\n",
    "                            fout.write('\\n'+curr_line)\n",
    "                            done.update(curr_line)\n",
    "                        curr_line=''\n",
    "                        curr_line+=line.strip()\n",
    "                    else:\n",
    "                        curr_line+=line.strip()\n",
    "                # add last line\n",
    "                if curr_line not in done:\n",
    "                    fout.write('\\n'+curr_line)\n",
    "                if file == metadata_files[-1]:\n",
    "                    fout.write('\\n')\n",
    "\n",
    "with open(f'{output_dir}/metadata/BVBRC_fixed.tsv', 'w') as fout:\n",
    "    # write header\n",
    "    fout.write('Genome ID\\tGenome Name\\tOther Names\\tNCBI Taxon ID\\tTaxon Lineage IDs\\tTaxon Lineage Names\\tSuperkingdom\\tKingdom\\tPhylum\\tClass\\tOrder\\tFamily\\tGenus\\tSpecies\\tGenome Status\\tStrain\\tSerovar\\tBiovar\\tPathovar\\tMLST\\tSegment\\tSubtype\\tH_type\\tN_type\\tH1 Clade Global\\tH1 Clade US\\tH5 Clade\\tpH1N1-like\\tLineage\\tClade\\tSubclade\\tOther Typing\\tCulture Collection\\tType Strain\\tReference\\tGenome Quality\\tCompletion Date\\tPublication\\tAuthors\\tBioProject Accession\\tBioSample Accession\\tAssembly Accession\\tSRA Accession\\tGenBank Accessions\\tSequencing Center\\tSequencing Status\\tSequencing Platform\\tSequencing Depth\\tAssembly Method\\tChromosome\\tPlasmids\\tContigs\\tSize\\tGC Content\\tContig L50\\tContig N50\\tTRNA\\tRRNA\\tMat Peptide\\tCDS\\tCoarse Consistency\\tFine Consistency\\tCheckM Contamination\\tCheckM Completeness\\tGenome Quality Flags\\tIsolation Source\\tIsolation Comments\\tCollection Date\\tCollection Year\\tSeason\\tIsolation Country\\tGeographic Group\\tGeographic Location\\tOther Environmental\\tHost Name\\tHost Common Name\\tHost Gender\\tHost Age\\tHost Health\\tHost Group\\tLab Host\\tPassage\\tOther Clinical\\tAdditional Metadata\\tComments\\tDate Inserted\\tDate Modified\\n')\n",
    "    done={'',} # keep track of samples already added\n",
    "    for file in metadata_files:\n",
    "        if file[1]=='BVBRC':\n",
    "            with open(file[0], 'r') as fin:\n",
    "                fin.readline() # skip header\n",
    "                for line in fin:\n",
    "                    if line not in done:\n",
    "                        fout.write('\\t'.join([l.strip('\"') for l in line.strip().split('\\t')])+'\\n')\n",
    "                        done.update(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify and filter metdata\n",
    "There's lots of fields we don't really need in the metadata and GISAID and BVBRC use completely different formats. To avoid duplicate entries, we'll also be going through the deduplicated sequence set and ensure one entry/sequence.\n",
    "\n",
    "We also take this opportunity to correct common (and easily detectable) metadata errors such as American-formatted dates and inverted countries/continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata will be read into dicts:\n",
    "# strain_name:date  host    host group   continent   country passage history\n",
    "\n",
    "report_data=[]\n",
    "mdata_counts={'total':0,\n",
    "              'no date':0,\n",
    "              'no data':0,\n",
    "              'not in species':0}\n",
    "# read in species list\n",
    "# expanding list of species sorted into groups\n",
    "missing_species=[]\n",
    "with open(species_list, 'r') as f:\n",
    "    species_dict={line.strip().split('\\t')[1]:line.split('\\t')[0] for line in f}\n",
    "\n",
    "# read in all GISAID data\n",
    "md_gisaid={}\n",
    "with open(f'{output_dir}/metadata/GISAID_fixed.tsv', 'r') as f:\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        l=line.split('\\t')\n",
    "\n",
    "        # figure out species group\n",
    "        species=remove_illegal_chars(l[16]).lower()\n",
    "        if species in species_dict.keys():\n",
    "            species_group = species_dict[species]\n",
    "        else:\n",
    "            species_group='unknown'\n",
    "            missing_species.append(species)\n",
    "     \n",
    "        md_gisaid[remove_illegal_chars(l[11])]=[l[24], l[16], species_group, l[15].split(' / ')[0].strip('\"'), l[15].split(' / ')[1].strip('\"') if len(l[15].split(' / '))>1 else '', l[14]]\n",
    "\n",
    "# read in all BVBRC data\n",
    "md_bvbrc={}\n",
    "with open(f'{output_dir}/metadata/BVBRC_fixed.tsv', 'r') as f:\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        l=line.split('\\t')\n",
    "\n",
    "        # figure out species group\n",
    "        species=remove_illegal_chars(l[74]).lower().split('-_gender')[0]\n",
    "        if species in species_dict.keys():\n",
    "            species_group = species_dict[species]\n",
    "        else:\n",
    "            species_group='unknown'\n",
    "            missing_species.append(species)\n",
    "\n",
    "        md_bvbrc[remove_illegal_chars(l[15])]=[l[67], l[74], species_group, l[71].strip('\"'), l[70].strip('\"'), l[81]]\n",
    "        \n",
    "# go through sequences, grab mdata, CORRECT, and put to file\n",
    "with open(f'{output_dir}/metadata/mdata.tsv', 'w') as f:\n",
    "    f.write('name\\tdate\\thost\\thost group\\tcontinent\\tcountry\\tpassage history\\n')\n",
    "    for record in SeqIO.parse(f'{output_dir}/temp/id_filtered.fasta', 'fasta'):\n",
    "        mdata_counts['total']+=1\n",
    "        strain=remove_illegal_chars(record.id.split('|')[2])\n",
    "        if '**gb' in record.id:\n",
    "        # use BCBRV metadata\n",
    "            if strain not in md_bvbrc.keys():\n",
    "                print(f'no BVBRC entry for {record.id}')\n",
    "                report_data.append(f'{strain}: no metadata found in {output_dir}/metadata/BVBRC_fixed.tsv')\n",
    "                mdata='\\t\\t\\t\\t\\t'\n",
    "                mdata_counts['no data']+=1\n",
    "            else:\n",
    "                if corr_mdata==True:\n",
    "                    mdata, report_datum=correct_mdata(strain,md_bvbrc[strain])\n",
    "                    report_data+=report_datum\n",
    "                    if mdata.split('\\t')[0]=='XXXX-XX-XX':\n",
    "                        mdata_counts['no date']+=1\n",
    "                    if mdata.split('\\t')[2]=='unknown':\n",
    "                        mdata_counts['not in species']+=1\n",
    "                else:\n",
    "                    mdata='\\t'.join(md_bvbrc[strain])\n",
    "        else:\n",
    "        # use GISAID metadata\n",
    "            if strain not in md_gisaid.keys():\n",
    "                print(f'no GISAID entry for {record.id}')\n",
    "                report_data.append(f'{strain}: no metadate found in {output_dir}/metadata/GISAID_fixed.tsv')\n",
    "                mdata='\\t\\t\\t\\t\\t'\n",
    "                mdata_counts['no data']+=1\n",
    "            else:\n",
    "                if corr_mdata==True:\n",
    "                    mdata, report_datum=correct_mdata(strain,md_gisaid[strain])\n",
    "                    report_data+=report_datum\n",
    "                    if mdata.split('\\t')[0]=='XXXX-XX-XX':\n",
    "                        mdata_counts['no date']+=1\n",
    "                    if mdata.split('\\t')[2]=='unknown':\n",
    "                        mdata_counts['not in species']+=1\n",
    "                else:\n",
    "                    mdata='\\t'.join(md_gisaid[strain])\n",
    "        f.write(f'{remove_illegal_chars(record.id)}\\t{mdata}\\n')\n",
    "\n",
    "if missing_species!=[] and report_data==True:\n",
    "    with open(f'{output_dir}/reports/missing_species.tsv', 'r') as f:\n",
    "        for species in set(missing_species):\n",
    "            f.write(f'\\t{species}\\n')\n",
    "            print(f'no species list entry for {species}')\n",
    "\n",
    "\n",
    "\n",
    "if report_data != [] and report_files==True:\n",
    "    with open(f'{output_dir}/reports/metadata_correction.txt', 'w') as f:\n",
    "        for report in report_data:\n",
    "            f.write(report+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final filtering of sequences and ORF finding\n",
    "Here we do a bunch of things at once:\n",
    "- Find ORFs for all sequences and filter out sequences without stop codon or with short ORFs\n",
    "- Filter illegal characters from sequence names\n",
    "- Filter out sequences with no medata or no date information (optional)\n",
    "- Filter out 100% identical ORFs and only keep the earliest occurence (optional)\n",
    "- Filter out ORFs with too many inderterminate positions (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initializing counters/lists\n",
    "i = 0\n",
    "orfgood = [] # these are ORFs with a start and a stop\n",
    "orfbad = [] # these are ORFS with a start and a stop but smaller than 90% of the average ORF or\n",
    "            # or ORFs without stop codon\n",
    "\n",
    "orf_counters={'nostop':0,\n",
    "              'short':0,\n",
    "              'no_mdata':0,\n",
    "              'too_many_ind':0,\n",
    "              'dupes':0,\n",
    "              'kept':0}\n",
    "\n",
    "# read in sequences\n",
    "in_seqs=SeqIO.index(f'{output_dir}/temp/id_filtered.fasta', \"fasta\")\n",
    "\n",
    "# read in metadata\n",
    "mdata={}\n",
    "with open(f'{output_dir}/metadata/mdata.tsv', 'r') as f:\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        l=line.strip().split('\\t')\n",
    "        mdata[l[0]]=l[1:]\n",
    "\n",
    "for record in SeqIO.parse(f'{output_dir}/temp/id_filtered.fasta', 'fasta'): # iterating through sequences\n",
    "    if purge_nomdata == True and remove_illegal_chars(record.id) not in mdata.keys():\n",
    "    # no metadata\n",
    "        orfbad.append((record.id, 'no_mdata', record.seq))\n",
    "        orf_counters['no_mdata']+=1\n",
    "    elif purge_nomdata == True and mdata[remove_illegal_chars(record.id)][0]== 'XXXX-XX-XX':\n",
    "    # no date\n",
    "        orfbad.append((record.id, 'no_date', record.seq))\n",
    "        orf_counters['no_mdata']+=1\n",
    "    else:\n",
    "    # everything ok, proceed to find ORFs\n",
    "        seq = record.seq[record.seq.upper().find(\"ATG\"):] # trim 5' end by finding first ATG and slicing \n",
    "        \n",
    "        while len(seq) % 3 != 0: # making seq length a multiple of 3 so biopython doesn't yell at me (Ns will get cut off in next step)\n",
    "            seq += \"N\"\n",
    "        \n",
    "        seq = seq[:len(seq.translate(to_stop=True))*3+3] # trim 3' end by slicing everything after the stop codon\n",
    "\n",
    "        # sometimes the first ATG is not the right one, this part also tests the second ATG\n",
    "        # if the orf from the first was too short\n",
    "        if len(seq) < min_orf_len: \n",
    "            seq2 = record.seq[record.seq[record.seq.upper().find(\"ATG\")+3:].upper().find('ATG')+record.seq.upper().find(\"ATG\")+3:]\n",
    "            while len(seq2) % 3 !=0 :\n",
    "                seq2 += \"N\"\n",
    "            seq2 = seq2[:len(seq2.translate(to_stop=True))*3+3]\n",
    "            # if this ORF is longer, replace the previous small one by this one\n",
    "            if len(seq2) > len(seq):\n",
    "                seq = seq2\n",
    "\n",
    "        if purge_nostop==True and seq.translate()[-1:] != \"*\": # check for stop codon at the end of ORF\n",
    "            orfbad.append((record.id,'nostop', seq))\n",
    "            orf_counters['nostop']+=1\n",
    "        else: # either we're not checking or there is a stop codon\n",
    "            orfgood.append((record.id,seq))\n",
    "\n",
    "## sort out short ORFs (90% cut-off)\n",
    "# 90% is arbitrary, adjust as desired\n",
    "orf_cutoff_nts = sum([len(orf[1]) for orf in orfgood])/len(orfgood)*orf_cutoff\n",
    "\n",
    "# turn orfgood inside out to group ids from identical sequences:\n",
    "orfids=defaultdict(list)\n",
    "for orf in orfgood:\n",
    "    orfids[orf[1].upper()].append(orf[0])\n",
    "\n",
    "replaced=[]\n",
    "with open(f'{output_dir}/{name}_unique_orfs.fasta', \"w\") as f:\n",
    "    for seq in orfids.keys(): # iterate through unique sequences\n",
    "        if inderterminate_ratio(seq.upper())<max_indeterminate:\n",
    "        # check indeterminate character ratio\n",
    "            # check for length, if bad set aside\n",
    "            if len(seq) < orf_cutoff_nts:\n",
    "            # check for length, if bad set aside\n",
    "                for seqid in orfids[seq]:\n",
    "                    orfbad.append((seqid, 'short', seq))\n",
    "                    orf_counters['short']+=1\n",
    "            else:\n",
    "            # length is ok\n",
    "                if purge_unique == True:\n",
    "                    if len(orfids[seq])>1:\n",
    "                    # not unique sequence, figure out which to keep\n",
    "                        v=[remove_illegal_chars(seqid) for seqid in orfids[seq]]\n",
    "                        # find the sequence with earliest date\n",
    "                        # for incomplete dates, first day of first month is taken\n",
    "                        earliest_id =v[[datetime.fromisoformat('-01'.join(mdata[seqid][0].split('-XX'))) for seqid in v].index(min([datetime.fromisoformat('-01'.join(mdata[seqid][0].split('-XX'))) for seqid in v]))]\n",
    "                        v.remove(earliest_id)\n",
    "                        f.write(f\">{earliest_id}\\n{seq}\\n\")\n",
    "                        replaced.append(f\"{earliest_id}\\t{','.join(v) if len(v)>1 else v[0]}\\n\")\n",
    "                        orf_counters['kept']+=1\n",
    "                        orf_counters['dupes']+=len(v)\n",
    "                    else:\n",
    "                        f.write(f'>{orfids[seq][0]}\\n{seq}\\n')\n",
    "                        orf_counters['kept']+=1\n",
    "\n",
    "                else:\n",
    "                # we don't care about uniques\n",
    "                    for seqid in orfids[seq]:\n",
    "                        f.write(f\">{seqid}\\n{seq}\\n\")\n",
    "        else:\n",
    "        # too many indeterminate characters\n",
    "            orfbad.append((orf[0], f'too_many_ind-{inderterminate_ratio(orf[1].upper())}', orf[1]))\n",
    "            orf_counters['too_many_ind']+=1\n",
    "\n",
    "\n",
    "# reporting on data\n",
    "print(f'looked for orfs in {sum(orf_counters.values())} sequences:')\n",
    "print(f\"\\t{sum(orf_counters.values())-orf_counters['kept']:>6} ({(sum(orf_counters.values())-orf_counters['kept'])/sum(orf_counters.values())*100:5.2f}%) sequences discarded\")\n",
    "if purge_nomdata==True:\n",
    "    print(f\"\\t\\t{orf_counters['no_mdata']:>6} ({orf_counters['no_mdata']/sum(orf_counters.values())*100:5.2f}%) had no associated metadata or date\")\n",
    "print(f\"\\t\\t{orf_counters['short']:>6} ({orf_counters['short']/sum(orf_counters.values())*100:5.2f}%) were shorter than {orf_cutoff*100:.0f}% of the average ({orf_cutoff_nts:.0f})\")\n",
    "if purge_nostop==True:\n",
    "    print(f\"\\t\\t{orf_counters['nostop']:>6} ({orf_counters['nostop']/sum(orf_counters.values())*100:5.2f}%) had no stop codon\")\n",
    "if max_indeterminate!=1:\n",
    "    print(f\"\\t\\t{orf_counters['too_many_ind']:>6} ({orf_counters['too_many_ind']/sum(orf_counters.values())*100:5.2f}%) had over {max_indeterminate*100:.0f}% indeterminate posiions\")\n",
    "if purge_unique==True:\n",
    "    print(f\"\\t\\t{orf_counters['dupes']:>6} ({orf_counters['dupes']/sum(orf_counters.values())*100:5.2f}%) were not unique\")\n",
    "print(f\"\\t{orf_counters['kept']:>6} ({(orf_counters['kept'])/sum(orf_counters.values())*100:5.2f}%) sequences kept and written to {output_dir}/{name}_unique_orfs.fasta\")\n",
    "\n",
    "if report_files==True:\n",
    "    # sift through all the stuff we exluded\n",
    "    orfbad_output=[]\n",
    "    with open(f'{output_dir}/reports/bad_orfs.tsv', 'w') as f:\n",
    "        for orf in orfbad:\n",
    "            if orf[1] in ('nostop', 'short'):\n",
    "                orfbad_output.append(orf)\n",
    "                detail=len(orf[2])\n",
    "            elif orf[1]=='too_many_ind':\n",
    "                detail=f'{inderterminate_ratio(orf[2]):.2f}'\n",
    "            else:\n",
    "                detail=''\n",
    "            f.write(f'{orf[0]}\\t{orf[1]}\\t{detail}\\n')\n",
    "    \n",
    "    if orfbad_output!=[] and write_badorf==True:\n",
    "        with open(f'{output_dir}/temp/orfs_bad.fasta', \"w\") as f2:\n",
    "            for item in orfbad_output:\n",
    "                f2.write(f'>{item[0]}_{item[1]}\\n{item[2]}\\n')\n",
    "                f2.write(f'>{item[0]}\\n{str(in_seqs[item[0]].seq)}\\n')\n",
    "\n",
    "    # report on replacements\n",
    "    with open(f'{output_dir}/reports/replaced.tsv', 'w') as f:\n",
    "        for item in replaced:\n",
    "            f.write(item)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should now be ready for alignment tree building.\n",
    "\n",
    "Of course, there's always more that can be done, starting with getting some stats on the whole process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Some info and stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to output a file that sums up what was done to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_dir}/parameters.txt', 'w') as f:\n",
    "    f.write(f'dataset {name} was cleaned up using pépinière v{version} on {date.today()}\\n')\n",
    "    if report_files==True:\n",
    "        f.write(f\"reports were generated and written to {output_dir}/reports\\n\")\n",
    "    else:\n",
    "        f.write('no reports were generated\\n')\n",
    "\n",
    "\n",
    "    f.write(f'\\n{\"_\"*60}\\n')\n",
    "    f.write('INPUT SEQUENCES\\n')\n",
    "    f.write(f'input files were: {seq_files[0][0]} ({seq_files[0][1]} format)\\n')\n",
    "    for seq_file in seq_files[1:]:\n",
    "        f.write(f\"{' '*18}{seq_file[0]} ({seq_file[1]} format)\\n\")\n",
    "    f.write(f'\\nfound and removed {seq_counts[\"total\"]-seq_counts[\"dedup\"]} duplicate sequences based on accession/EPI numbers ({(seq_counts[\"total\"]-seq_counts[\"dedup\"])/seq_counts[\"total\"]*100:.2f}%)\\n')\n",
    "    f.write(f\"final set contains {db_counts['gisaid']+db_counts['bvbrc']} sequences,\"+\n",
    "        f\" of which {db_counts['gisaid']} from GISAID ({db_counts['gisaid']/(db_counts['gisaid']+db_counts['bvbrc'])*100:.2f}%)\"+\n",
    "        f\" and {db_counts['bvbrc']} from BVBRC ({db_counts['bvbrc']/(db_counts['bvbrc']+db_counts['gisaid'])*100:.2f}%),\"+\n",
    "        f\" {db_counts['both']} of which were also in GISAID ({db_counts['both']/db_counts['bvbrc']*100:.2f}%)\\n\")\n",
    "    \n",
    "\n",
    "    f.write(f'\\n{\"_\"*60}\\n')\n",
    "    f.write('INPUT METADATA\\n')\n",
    "    f.write(f'\\nmetadata files were: {metadata_files[0][0]} ({metadata_files[0][1]} format)\\n')\n",
    "    for metadata_file in metadata_files[1:]:\n",
    "        f.write(f\"{' '*21}{metadata_file[0]} ({metadata_file[1]} format)\\n\")\n",
    "    f.write(f\"\\nmetadata was{' not' if corr_mdata==False else ''} corrected and consolidated to {output_dir}/metadata/mdata.tsv\\n\")\n",
    "    f.write(f\"species list used was {species_list} containing {len(species_dict)} entries but missing {len(set(missing_species))} entries\\n\")\n",
    "    f.write(f\"\\nof {mdata_counts['total']} processed metadata entries:\\n\")\n",
    "    f.write(f\"\\t{mdata_counts['no data']:>5} had no metadata info{' '*6}({mdata_counts['no data']/mdata_counts['total']*100:5.2f}%)\\n\")\n",
    "    f.write(f\"\\t{mdata_counts['no date']:>5} had no date info{' '*10}({mdata_counts['no date']/mdata_counts['total']*100:5.2f}%)\\n\")\n",
    "    f.write(f\"\\t{mdata_counts['not in species']:>5} had no species group info ({mdata_counts['not in species']/mdata_counts['total']*100:5.2f}%)\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    f.write(f'\\n{\"_\"*60}\\n')\n",
    "    f.write('ORF TRIMMING\\n')\n",
    "    f.write(f'looked for orfs in {sum(orf_counters.values())} sequences:\\n')\n",
    "    f.write(f\"\\t{sum(orf_counters.values())-orf_counters['kept']:>6} ({(sum(orf_counters.values())-orf_counters['kept'])/sum(orf_counters.values())*100:5.2f}%) sequences discarded\\n\")\n",
    "    f.write(f\"\\t\\t{orf_counters['short']:>6} ({orf_counters['short']/sum(orf_counters.values())*100:5.2f}%) were shorter than {orf_cutoff*100:.0f}% of the average ({orf_cutoff_nts:.0f})\\n\")\n",
    "    if purge_nomdata==True:\n",
    "        f.write(f\"\\t\\t{orf_counters['no_mdata']:>6} ({orf_counters['no_mdata']/sum(orf_counters.values())*100:5.2f}%) had no associated metadata or date\\n\")\n",
    "    else:\n",
    "        f.write('\\t\\tsequences without metadata were not removed from the dataset\\n')\n",
    "    if purge_nostop==True:\n",
    "        f.write(f\"\\t\\t{orf_counters['nostop']:>6} ({orf_counters['nostop']/sum(orf_counters.values())*100:5.2f}%) had no stop codon\\n\")\n",
    "    else:\n",
    "        f.write('\\t\\tORFs without stop codon were not removed from the dataset\\n')\n",
    "    if max_indeterminate!=1:\n",
    "        f.write(f\"\\t\\t{orf_counters['too_many_ind']:>6} ({orf_counters['too_many_ind']/sum(orf_counters.values())*100:5.2f}%) had over {max_indeterminate*100:.0f}% indeterminate posiions\\n\")\n",
    "    else:\n",
    "        f.write('\\t\\tORFs with indeterminate positions were not removed from the dataset\\n')\n",
    "    if purge_unique==True:\n",
    "        f.write(f\"\\t\\t{orf_counters['dupes']:>6} ({orf_counters['dupes']/sum(orf_counters.values())*100:5.2f}%) were not unique\\n\")\n",
    "    else:\n",
    "        f.write('\\t\\tduplicate ORFs were not removed from the dataset\\n')\n",
    "    f.write(f\"\\t{orf_counters['kept']:>6} ({(orf_counters['kept'])/sum(orf_counters.values())*100:5.2f}%) sequences kept and written to {output_dir}/{name}_unique_orfs.fasta\\n\")\n",
    "    f.write(f'\\nfinal dataset contains {orf_counters[\"kept\"]} of {seq_counts[\"total\"]} sequences ({orf_counters[\"kept\"]/seq_counts[\"total\"]*100:.2f}%)')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Treework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "417915ee3369acde11c6b3933ba362a9526ec24c3d87f73bff43ddb0d5881961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
