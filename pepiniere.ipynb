{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard packages\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from shlex import quote\n",
    "# non-standard packages, get these via conda\n",
    "from Bio import SeqIO\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    import plotly.io as pio\n",
    "    pio.renderers.default = 'notebook'\n",
    "    plotting=True\n",
    "except ModuleNotFoundError:\n",
    "    print(f'did not find plotting packages, no stats will be run')\n",
    "    plotting=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pépinière\n",
    "This notebook contains a bunch of scripts that can process and clean up data from GISAID/BVBRC. It takes any number of fasta files and combines them into one final file. It requires metadata with time, species and continent of isolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the fasta files and metadata files here and specify which website they were downloaded from. The sequences from the different files will be concatenated into a single file and duplicates will be removed during the process.\n",
    "\n",
    "GISAID files should be dowloaded with fasta header format: Type|  DNA INSDC   | Isolate name   | DNA Accession no., YYYY-MM-DD format and both space replacement/removal checked.<p>\n",
    "BVBRC files should be downloaded in standard format and will be reformatted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple of files containing the sequences\n",
    "# (<path>, <GISAID/BVBRC>)\n",
    "seq_files=()\n",
    "\n",
    "# tuple of files containing metadata\n",
    "# (<path>, <GISAID/BVBRC>)\n",
    "metadata_files=()\n",
    "\n",
    "# path the species list if used\n",
    "species_list='/Users/threecats/Data/git/pepiniere/species_list_sorted.txt'\n",
    "\n",
    "# string pointing at the directory where you want the data\n",
    "output_dir=''\n",
    "\n",
    "# name of final dataset\n",
    "name=''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set up\n",
    "Import packages, set up functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TOGGLES\n",
    "# these two steps can be very time consuming depending on the size of the dataset\n",
    "# you may also want to use other software to analyze your dataset, though MAFFT \n",
    "# and fasttree are probably fine for a first look at the data\n",
    "# !!! mafft, fasttree and treetime need to be installed in your python environment\n",
    "# you can get them all via conda.\n",
    "\n",
    "# should we align the data with MAFFT?\n",
    "toggle_align=True\n",
    "# how many threads to use for alignment? This can speed it up significantly, \n",
    "# but should note exceed teh number of cores your computer has\n",
    "threads_align=8\n",
    "# should we make a fasttree?\n",
    "toggle_tree=True\n",
    "# should we make a treetime?\n",
    "toggle_treetime=True\n",
    "\n",
    "## REPORTING PARAMETERS\n",
    "# should we output report files?\n",
    "report_files = True\n",
    "# identical sequences isolated *more* than these many years apart will be reported on\n",
    "year_gap_threshold = 1\n",
    "##\n",
    "\n",
    "## DATASET CURATION OPTIONS\n",
    "# should we try to correct common metadata errors?\n",
    "corr_mdata = True\n",
    "# should we throw out all files without minimal metadata (ie date)\n",
    "purge_nomdata = True\n",
    "# should we only keep on copy of all unique sequences?\n",
    "purge_unique = True\n",
    "# should we remove ORFs without stop codon?\n",
    "purge_nostop = True\n",
    "###\n",
    "\n",
    "## ORF TRIMMING PARAMETERS\n",
    "# under what length should we look for a second ORF\n",
    "min_orf_len=1000 # (nts)\n",
    "# what is the lowest ORF length acceptable\n",
    "orf_cutoff=0.9 # (relative to average)\n",
    "# sequences with proportion of undeterminate characters over the threshold will be removed\n",
    "# set to 1 to not remove sequences with indetermined positions\n",
    "max_indeterminate=1\n",
    "# make a file with all short/nostop orfs and the initial sequence?\n",
    "write_badorf=False\n",
    "##\n",
    "\n",
    "# a list of accepted continent names for metadata\n",
    "# if one of these cannot be found, unknown is reported\n",
    "continents=('North_America','South_America','Europe','Oceania','Africa','Asia','Antarctica')\n",
    "\n",
    "seq_counts={}\n",
    "for folder in ('temp','data','tree','metadata','annotations'):\n",
    "    if not os.path.exists(f'{output_dir}/{folder}/'):\n",
    "        os.mkdir(f'{output_dir}/{folder}/')\n",
    "if not os.path.isfile(f'{output_dir}/metadata/override.tsv'):\n",
    "    with open(f'{output_dir}/metadata/override.tsv', 'w') as f:\n",
    "        f.write('this file contains overrides to the metadata, it is loaded last and overwrites anything from the other files\\n')\n",
    "\n",
    "if not os.path.isfile(f'{output_dir}/metadata/blacklist.txt'):\n",
    "    with open(f'{output_dir}/metadata/blacklist.txt', 'w') as f:\n",
    "        f.write('this file contains IDs that should be taken out of the final dataset\\n')\n",
    "\n",
    "if report_files==True:\n",
    "    if not os.path.exists(f'{output_dir}/reports/'):\n",
    "        os.mkdir(f'{output_dir}/reports/')\n",
    "\n",
    "version='23-02-16'\n",
    "# initialize modules as not having been run\n",
    "alignment_run=False\n",
    "tree_run=False\n",
    "treetime_run=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function removes illegal characters from strain names and metadata\n",
    "# also strips whitespaces from front and back\n",
    "def remove_illegal_chars(id):\n",
    "    id=id.strip(' ')\n",
    "    id = '_'.join(id.split(' '))\n",
    "    # there probs is a more elegant way to do this, but its fast enough right now\n",
    "    for char in ('(',')',';',':','.',\",\",\"'\",'?','（','）',' '):\n",
    "        id = '-'.join(id.split(char))\n",
    "    return id\n",
    "\n",
    "# this function fixes common errors in metadata and returns a formatted string\n",
    "# mdata should be list date  host    host group    continent   country passage history\n",
    "# species_dict should be dict species:group\n",
    "def correct_mdata(strain, mdata):\n",
    "    report_data=[]\n",
    "    fixed_date, fixed_continent = '',''\n",
    "    date_cutoff=2 # decade cut-off for 19XX or 20XX date guessing\n",
    "\n",
    "    # fixing date\n",
    "    fixed_date, report_datum=fix_date(mdata[0])\n",
    "    if fixed_date=='XXXX-XX-XX':\n",
    "    # no date provided, or date grivesouly misformatted\n",
    "    # try to find date in strain name, else just keep as missing and output warning\n",
    "        strain_date4, strain_date2=strain.rstrip('/').split('/')[-1][:4], strain.strip('/').split('/')[-1][:2]\n",
    "        if len(strain_date4)==4 and strain_date4.isnumeric():\n",
    "        # 4 digit number at end of strain name\n",
    "            fixed_date=strain_date4+'-XX-XX'\n",
    "        elif strain_date2.isnumeric():\n",
    "        # no 4 digit, but 2 digit number at end of strain name\n",
    "            if int(strain_date2[0])<=date_cutoff:\n",
    "                fixed_date=f'20{strain_date2}-XX-XX'\n",
    "            else:\n",
    "                fixed_date=f'19{strain_date2}-XX-XX'\n",
    "\n",
    "        if fixed_date != 'XXXX-XX-XX':\n",
    "            report_data.append(f'{strain}: date \"{mdata[0]}\" could not be parsed, guessed as \"{fixed_date}\"')\n",
    "        else:\n",
    "            print(f'warning, no date for {strain}')\n",
    "            report_data.append(f'WARNING no date for {strain}')\n",
    "    elif report_datum != '':\n",
    "        report_data.append(f'{strain}: date \"{mdata[0]}\" is wrong format, replaced by \"{fixed_date}\"')\n",
    "\n",
    "    # fixing geography\n",
    "    if remove_illegal_chars(mdata[3]) not in continents and remove_illegal_chars(mdata[4]) in continents:\n",
    "        fixed_continent =f'{remove_illegal_chars(mdata[4])}\\t{remove_illegal_chars(mdata[3])}'\n",
    "        report_data.append(f'{strain}: fixed continent: \"{mdata[3]}/{mdata[4]}\" -> \"{mdata[4]}/{mdata[3]}\"')\n",
    "    elif remove_illegal_chars(mdata[3]) not in continents and remove_illegal_chars(mdata[4]) not in continents:\n",
    "        fixed_continent = f'unknown\\t{remove_illegal_chars(mdata[4])}' \n",
    "        report_data.append(f'{strain}: fixed continent: \"{mdata[3]}/{mdata[4]}\" -> \"unknown/{mdata[4]}\"')\n",
    "    else:\n",
    "        fixed_continent = f'{remove_illegal_chars(mdata[3])}\\t{remove_illegal_chars(mdata[4])}'\n",
    "    return(f'{fixed_date}\\t{mdata[1]}\\t{mdata[2]}\\t{fixed_continent}\\t{mdata[5]}', report_data)\n",
    "\n",
    "def fix_date(date):\n",
    "    months=('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec')\n",
    "    d=date.split('-')\n",
    "    date_fixed=date\n",
    "    if ''.join(d).isnumeric():\n",
    "    # only numbers, good sign\n",
    "        if len(date)<10:\n",
    "        # lower precision date\n",
    "            if len(d[0])!=4:\n",
    "            # MM-YYYY format probably??? turn around\n",
    "                date_fixed=f\"{d[1]}-{d[0]}\"\n",
    "        elif int(d[1])>12:\n",
    "        # looks like 10 character date but in US format\n",
    "            date_fixed=f'{d[0]}-{d[2]}-{d[1]}'\n",
    "\n",
    "        elif len(d[2])==4:\n",
    "        # date in DD-MM-YYYY\n",
    "            date_fixed=f\"{d[2]}-{d[1]}-{d[0]}\"   \n",
    "    elif len(d)>1:\n",
    "    # letters in date, uh oh, but at least several fields\n",
    "        if len(d)==3 and  d[1].lower() in months:\n",
    "        # three letter code for month DD-MMM-YYYY or YYYY-MMM-DDDformat\n",
    "            if len(d[0])==4:\n",
    "            # YYYY-MMM-DD\n",
    "                date_fixed=f'{d[0]}-{months.index(d[1].lower())+1:02}-{d[2]}'\n",
    "            elif len(d[2])==4:\n",
    "            # DD-MMM-YYYY\n",
    "                date_fixed=f'{d[2]}-{months.index(d[1].lower())+1:02}-{d[0]}'\n",
    "            else:\n",
    "                date_fixed='XXXX-XX-XX'\n",
    "        elif len(d)==2:\n",
    "            if d[0].lower() in months and len(d[1])==4:\n",
    "            # MMM-YYYY\n",
    "                date_fixed=f'{d[1]}-{months.index(d[0].lower())+1:02}-XX'\n",
    "            elif d[1].lower() in months and len(d[0])==4:\n",
    "            # YYYY-MMM\n",
    "                date_fixed=f'{d[0]}-{months.index(d[1].lower())+1:02}-XX'\n",
    "            else:\n",
    "                date_fixed='XXXX-XX-XX'\n",
    "        else:  \n",
    "        # no clue, just try to grab a year from strain name\n",
    "            for part in d:\n",
    "                if len(part)==4 and part.isnumeric():\n",
    "                # this one should be the year, else would not have 4 digits\n",
    "                    date_fixed=part+'-XX-XX'\n",
    "                    break\n",
    "            else:\n",
    "            # can't find a 4 digit year, either missing date or grievously misformatted\n",
    "                date_fixed='XXXX-XX-XX'\n",
    "    else:\n",
    "    # not numeric and just one field?! no clue\n",
    "        date_fixed='XXXX-XX-XX'\n",
    "\n",
    "    # everything should be numbers or XXXX by now, just pad it all in case it's needed\n",
    "    while len(date_fixed)<10:\n",
    "        date_fixed+='-XX'\n",
    "\n",
    "    return(date_fixed, f'\"{date}\" -> \"{date_fixed}\"' if date!=date_fixed.rstrip('-XX') else '')\n",
    "\n",
    "def inderterminate_ratio(seq):\n",
    "    return(sum([0 if c in ('A','T','C','G') else 1 for c in seq])/len(seq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merging files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming\n",
    "Standardizing fasta id format into a GISAID-like \\<subtype\\>|\\<accession number\\>|\\<strain name\\>. For GISAID, sequences without accession number will have the EPI number instead\n",
    "\n",
    "Also concatenates the sequences into a single temp file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize counter\n",
    "seq_counts['total']=0\n",
    "\n",
    "with open(f'{output_dir}/temp/{name}_all.fasta','w') as fout:\n",
    "    for file in seq_files:\n",
    "        \n",
    "        if file[1]=='GISAID':\n",
    "            for record in SeqIO.parse(file[0], 'fasta'):\n",
    "                rec_split=record.id.split('|')\n",
    "                if rec_split[1]!='':\n",
    "                # has an accession number\n",
    "                    record_id=f'{rec_split[0][4:]}|{rec_split[1]}|{rec_split[2]}|'\n",
    "                else:\n",
    "                # has no accession number  \n",
    "                    record_id=f'{rec_split[0][4:]}|EPI{rec_split[3]}|{rec_split[2]}|'\n",
    "                fout.write(f'>{record_id}\\n{record.seq}\\n')\n",
    "                seq_counts['total']+=1\n",
    "       \n",
    "        elif file[1]=='BVBRC':\n",
    "            # easiest way to deal with these is actually reading in metadata\n",
    "            # the fasta is formatted by idiots  >:(\n",
    "            # dict is accession_number:(subtype, strain name)\n",
    "            record_ids={}\n",
    "            for metafile in metadata_files:\n",
    "                if metafile[1]=='BVBRC':\n",
    "                    with open(metafile[0], 'r') as f:\n",
    "                        f.readline()\n",
    "                        for line in f:\n",
    "                            l=line.split('\\t')\n",
    "                            record_ids[l[43][1:-1]]=f\"{l[21][1:-1]}{'Nx' if len(l[21][1:-1])==2 else ''}|{l[43][1:-1]}|{'_'.join(l[15][1:-1].split(' '))}|**gb\"\n",
    "            for record in SeqIO.parse(file[0], 'fasta'):\n",
    "                fout.write(f'>{record_ids[record.id[5:]]}\\n{record.seq}\\n')\n",
    "                seq_counts['total']+=1\n",
    "        else:\n",
    "            print(f'cannot recognize {file[1]} format for {file[0]}, has to be GISAID or BVBRC, skipping')\n",
    "print(f'combined sequence total: {seq_counts[\"total\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First de-duplication round\n",
    "Here we only remove sequences that are 100% identical and with the same accession number/strain name. This is done in 2 rounds, first looks for duplicate accession numbers, second for duplicate strain names. I am assuming that all sequences with the same accession number are identical, I don't actually check for it. For strain names it gets more complicated since the sequences might differ.\n",
    "\n",
    "BVBRC agrees with Genbank while GISAID seems to make mistakes, so the former is prioritised. Additionally if a Genbank sequence is updated, GISAID does not update the entry while BVBRC does (I think? IRD used to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize counter\n",
    "seq_counts['dedup']=0\n",
    "blacklist=[]\n",
    "db_counts={'gisaid':0,'both':0,'bvbrc':0}\n",
    "report_data={}\n",
    "\n",
    "id_dict=defaultdict(list)\n",
    "with open(f'{output_dir}/temp/{name}_all.fasta','r') as f:\n",
    "    # get accession number from fasta id, ignore fasta without id\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            an=line.split('|')[1]\n",
    "            if an != '':\n",
    "                id_dict[an].append(line.strip().split('|',1)[1])\n",
    "\n",
    "for records in id_dict.values():\n",
    "    if len(records)>1:\n",
    "        if ''.join(records).count('**gb')==1:\n",
    "        # only one seq from BVBRC, keep that one\n",
    "            k=''\n",
    "            v=[]\n",
    "            db_counts['both']+=1\n",
    "            for record in records:\n",
    "                if '**gb' not in record:\n",
    "                    blacklist.append(record)\n",
    "                    v.append(record)\n",
    "                else:\n",
    "                    k=record\n",
    "            report_data[k]=v\n",
    "\n",
    "        elif ''.join(records).count('**gb')==0:\n",
    "        # only GISAID seqs, keep first\n",
    "            blacklist+=records[1:]\n",
    "            report_data[records[0]]=records[1:]\n",
    "        \n",
    "        else:\n",
    "        # several BVBRC, keep first\n",
    "            if ''.join(records).count('**gb')!=len(records):\n",
    "            # at least one GISAID\n",
    "                db_counts['both']+=1\n",
    "            for record in records:\n",
    "                if '**gb' in record:\n",
    "                    records.remove(record)\n",
    "                    report_data[record]=records\n",
    "                    break\n",
    "            blacklist+=records\n",
    "\n",
    "# weed out first batch\n",
    "seqs_to_keep=defaultdict(list)\n",
    "with open(f'{output_dir}/temp/{name}_id_filtered.fasta', 'w') as f:\n",
    "    for record in SeqIO.parse(f'{output_dir}/temp/{name}_all.fasta', 'fasta'):\n",
    "        if record.id.split('|',1)[1] in blacklist:\n",
    "        # found a blacklisted record, remove one entry in blacklist\n",
    "        # this is important if the same seq is present several times\n",
    "        # if we don't remove we keep none of them\n",
    "            blacklist.remove(record.id.split('|',1)[1])\n",
    "        else:\n",
    "            if '**gb' not in record.id:\n",
    "                db_counts['gisaid']+=1\n",
    "            else:\n",
    "                db_counts['bvbrc']+=1\n",
    "            seq_counts['dedup']+=1\n",
    "            f.write(f'>{record.id}\\n{record.seq}\\n')\n",
    "\n",
    "if report_files == True:\n",
    "# put out file with dupe info\n",
    "    with open(f'{output_dir}/reports/deduplication.txt', 'w') as f:\n",
    "        f.write('kept\\treplaces\\n')\n",
    "        for k,v in report_data.items():\n",
    "            f.write(f'{k}\\t{\",\".join(v)}\\n')\n",
    "\n",
    "print(f'found and removed {seq_counts[\"total\"]-seq_counts[\"dedup\"]} duplicate sequences ({(seq_counts[\"total\"]-seq_counts[\"dedup\"])/seq_counts[\"total\"]*100:.2f}%)')\n",
    "print(f\"final set contains {db_counts['gisaid']+db_counts['bvbrc']} sequences,\"+\n",
    "      f\" of which {db_counts['gisaid']} from GISAID ({db_counts['gisaid']/(db_counts['gisaid']+db_counts['bvbrc'])*100:.2f}%)\"+\n",
    "      f\" and {db_counts['bvbrc']} from BVBRC ({db_counts['bvbrc']/(db_counts['bvbrc']+db_counts['gisaid'])*100:.2f}%),\"+\n",
    "      f\" {db_counts['both']} of which were also in GISAID ({db_counts['both']/db_counts['bvbrc']*100:.2f}%)\")\n",
    "if report_files==True:\n",
    "    print(f'wrote IDs of duplicates to {output_dir}/reports/deduplication.txt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fixing and formatting metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GISAID metadata is downloaded as xlsx file and has been converted to tsv. Sadly some people put characters into the metadata that break the whole process so first we have to fix it.<p>\n",
    "BVBRC metadata just gets concatenated and the quotation marks removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_dir}/metadata/GISAID_fixed.tsv', 'w') as fout:\n",
    "    # write header\n",
    "    fout.write('Isolate_Id\\tPB2 Segment_Id\\tPB1 Segment_Id\\tPA Segment_Id\\tHA Segment_Id\\tNP Segment_Id\\tNA Segment_Id\\tMP Segment_Id\\tNS Segment_Id\\tHE Segment_Id\\tP3 Segment_Id\\tIsolate_Name\\tSubtype\\tLineage\\tPassage_History\\tLocation\\tHost\\tIsolate_Submitter\\tSubmitting_Lab\\tSubmitting_Sample_Id\\tAuthors\\tPublication\\tOriginating_Lab\\tOriginating_Sample_Id\\tCollection_Date\\tNote\\tUpdate_Date\\tSubmission_Date\\tAntigen_Character\\tAnimal_Vaccin_Product\\tAdamantanes_Resistance_geno\\tOseltamivir_Resistance_geno\\tZanamivir_Resistance_geno\\tPeramivir_Resistance_geno\\tOther_Resistance_geno\\tAdamantanes_Resistance_pheno\\tOseltamivir_Resistance_pheno\\tZanamivir_Resistance_pheno\\tPeramivir_Resistance_pheno\\tOther_Resistance_pheno\\tHost_Age\\tHost_Age_Unit\\tHost_Gender\\tPatient_Status\\tZip_Code\\tOutbreak\\tPathogen_Test_Info\\tIs_Vaccinated\\tHuman_Specimen_Source\\tAnimal_Specimen_Source\\tAnimal_Health_Status\\tDomestic_Status\\tPMID\\tPB2 INSDC_Upload\\tPB1 INSDC_Upload\\tPA INSDC_Upload\\tHA INSDC_Upload\\tNP INSDC_Upload\\tNA INSDC_Upload\\tMP INSDC_Upload\\tNS INSDC_Upload\\tHE INSDC_Upload\\tP3 INSDC_Upload')\n",
    "    done={'',} # keep track of samples already added\n",
    "    for file in metadata_files:\n",
    "        if file[1]=='GISAID':\n",
    "            with open(file[0], 'r') as fin:\n",
    "                curr_line=''\n",
    "                fin.readline() # skip header\n",
    "                for line in fin:\n",
    "                    if line.startswith('EPI_ISL_') and [l for l in line.split('\\t') if l != ''][1].startswith('EPI'):\n",
    "                    # a GISAID metadata line should start with EPI_ISL_ and the firsts non-empty field should start with EPI\n",
    "                        if curr_line not in done:\n",
    "                            fout.write('\\n'+curr_line)\n",
    "                            done.update(curr_line)\n",
    "                        curr_line=''\n",
    "                        curr_line+=line.strip()\n",
    "                    else:\n",
    "                        curr_line+=line.strip()\n",
    "                # add last line\n",
    "                if curr_line not in done:\n",
    "                    fout.write('\\n'+curr_line)\n",
    "                if file == metadata_files[-1]:\n",
    "                    fout.write('\\n')\n",
    "\n",
    "with open(f'{output_dir}/metadata/BVBRC_fixed.tsv', 'w') as fout:\n",
    "    # write header\n",
    "    fout.write('Genome ID\\tGenome Name\\tOther Names\\tNCBI Taxon ID\\tTaxon Lineage IDs\\tTaxon Lineage Names\\tSuperkingdom\\tKingdom\\tPhylum\\tClass\\tOrder\\tFamily\\tGenus\\tSpecies\\tGenome Status\\tStrain\\tSerovar\\tBiovar\\tPathovar\\tMLST\\tSegment\\tSubtype\\tH_type\\tN_type\\tH1 Clade Global\\tH1 Clade US\\tH5 Clade\\tpH1N1-like\\tLineage\\tClade\\tSubclade\\tOther Typing\\tCulture Collection\\tType Strain\\tReference\\tGenome Quality\\tCompletion Date\\tPublication\\tAuthors\\tBioProject Accession\\tBioSample Accession\\tAssembly Accession\\tSRA Accession\\tGenBank Accessions\\tSequencing Center\\tSequencing Status\\tSequencing Platform\\tSequencing Depth\\tAssembly Method\\tChromosome\\tPlasmids\\tContigs\\tSize\\tGC Content\\tContig L50\\tContig N50\\tTRNA\\tRRNA\\tMat Peptide\\tCDS\\tCoarse Consistency\\tFine Consistency\\tCheckM Contamination\\tCheckM Completeness\\tGenome Quality Flags\\tIsolation Source\\tIsolation Comments\\tCollection Date\\tCollection Year\\tSeason\\tIsolation Country\\tGeographic Group\\tGeographic Location\\tOther Environmental\\tHost Name\\tHost Common Name\\tHost Gender\\tHost Age\\tHost Health\\tHost Group\\tLab Host\\tPassage\\tOther Clinical\\tAdditional Metadata\\tComments\\tDate Inserted\\tDate Modified\\n')\n",
    "    done={'',} # keep track of samples already added\n",
    "    for file in metadata_files:\n",
    "        if file[1]=='BVBRC':\n",
    "            with open(file[0], 'r') as fin:\n",
    "                fin.readline() # skip header\n",
    "                for line in fin:\n",
    "                    if line not in done:\n",
    "                        fout.write('\\t'.join([l.strip('\"') for l in line.strip().split('\\t')])+'\\n')\n",
    "                        done.update(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify and filter metdata\n",
    "There's lots of fields we don't really need in the metadata and GISAID and BVBRC use completely different formats. To avoid duplicate entries, we'll also be going through the deduplicated sequence set and ensure one entry/sequence.\n",
    "\n",
    "We also take this opportunity to correct common (and easily detectable) metadata errors such as American-formatted dates and inverted countries/continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata will be read into dicts:\n",
    "# strain_name:date  host    host group   continent   country passage history\n",
    "\n",
    "report_data=[]\n",
    "mdata_counts={'total':0,\n",
    "              'no date':0,\n",
    "              'no data':0,\n",
    "              'not in species':0,\n",
    "              'overrides':0,\n",
    "              'overriden':0\n",
    "              }\n",
    "# read in species list\n",
    "# expanding list of species sorted into groups\n",
    "missing_species=[]\n",
    "with open(species_list, 'r') as f:\n",
    "    species_dict={line.strip().split('\\t')[0]:line.strip().split('\\t')[2] for line in f}\n",
    "\n",
    "# read in overrides\n",
    "overrides={}\n",
    "with open(f'{output_dir}/metadata/override.tsv', 'r') as f:\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        mdata_counts['overrides']+=1\n",
    "        l=line.strip().split('\\t')\n",
    "        overrides[l[0]]=l[1:]\n",
    "\n",
    "# read in all GISAID data\n",
    "md_gisaid={}\n",
    "with open(f'{output_dir}/metadata/GISAID_fixed.tsv', 'r') as f:\n",
    "    ln=1\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        ln+=1\n",
    "        l=line.split('\\t')\n",
    "        try:\n",
    "            # figure out species group\n",
    "            species=remove_illegal_chars(l[16]).lower()\n",
    "            if species in species_dict.keys():\n",
    "                species_group = species_dict[species]\n",
    "            else:\n",
    "                species_group='unknown'\n",
    "                missing_species.append(species)  \n",
    "\n",
    "            md_gisaid[remove_illegal_chars(l[11])]=[l[24], l[16], species_group, l[15].split(' / ')[0].strip('\"'), l[15].split(' / ')[1].strip('\"') if len(l[15].split(' / '))>1 else '', l[14]]\n",
    "        except IndexError:\n",
    "            print(f'warning, GISAID metadata line {ln} is too short (length {len(l)}), skipping ')\n",
    "\n",
    "# read in all BVBRC data\n",
    "md_bvbrc={}\n",
    "with open(f'{output_dir}/metadata/BVBRC_fixed.tsv', 'r') as f:\n",
    "    ln=1\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        ln+=1\n",
    "        l=line.split('\\t')\n",
    "        try:\n",
    "            # figure out species group\n",
    "            species=remove_illegal_chars(l[74]).lower().split('-_gender')[0]\n",
    "            if species in species_dict.keys():\n",
    "                species_group = species_dict[species]\n",
    "            else:\n",
    "                species_group='unknown'\n",
    "                missing_species.append(species)\n",
    "\n",
    "            md_bvbrc[remove_illegal_chars(l[15])]=[l[67], l[74], species_group, l[71].strip('\"'), l[70].strip('\"'), l[81]]\n",
    "        except IndexError:\n",
    "            print(f'warning, BVBRC metadata line {ln} is too short (length {len(l)}), skipping ')\n",
    "        \n",
    "# go through sequences, grab mdata, CORRECT, and put to file\n",
    "with open(f'{output_dir}/metadata/mdata.tsv', 'w') as f:\n",
    "    f.write('name\\tdate\\thost\\thost group\\tcontinent\\tcountry\\tpassage history\\n')\n",
    "    for record in SeqIO.parse(f'{output_dir}/temp/{name}_id_filtered.fasta', 'fasta'):\n",
    "        mdata_counts['total']+=1\n",
    "        strain=remove_illegal_chars(record.id.split('|')[2])\n",
    "\n",
    "        if remove_illegal_chars(record.id) in overrides.keys():\n",
    "        # there's an override for this sequence\n",
    "            mdata='\\t'.join(overrides[remove_illegal_chars(record.id)])\n",
    "            f.write(f'{remove_illegal_chars(record.id)}\\t{mdata}\\n')\n",
    "            mdata_counts['overriden']+=1\n",
    "            continue\n",
    "        if '**gb' in record.id:\n",
    "        # use BCBRV metadata\n",
    "            if strain not in md_bvbrc.keys():\n",
    "                print(f'no BVBRC entry for {record.id}')\n",
    "                report_data.append(f'WARNING {strain}: no metadata found in {output_dir}/metadata/BVBRC_fixed.tsv')\n",
    "                mdata='XXXX-XX-XX\\tunknown\\tunknown\\tunknown\\tunknown\\tunknown'\n",
    "                mdata_counts['no data']+=1\n",
    "            else:\n",
    "                if corr_mdata==True:\n",
    "                    mdata, report_datum=correct_mdata(strain,md_bvbrc[strain])\n",
    "                    report_data+=report_datum\n",
    "                    if mdata.split('\\t')[0]=='XXXX-XX-XX':\n",
    "                        mdata_counts['no date']+=1\n",
    "                    if mdata.split('\\t')[2]=='unknown':\n",
    "                        mdata_counts['not in species']+=1\n",
    "                else:\n",
    "                    mdata='\\t'.join(md_bvbrc[strain])\n",
    "        else:\n",
    "        # use GISAID metadata\n",
    "            if strain not in md_gisaid.keys():\n",
    "                print(f'no GISAID entry for {record.id}')\n",
    "                report_data.append(f'WARNING {strain}: no metadata found in {output_dir}/metadata/GISAID_fixed.tsv')\n",
    "                mdata='XXXX-XX-XX\\tunknown\\tunknown\\tunknown\\tunknown\\tunknown'\n",
    "                mdata_counts['no data']+=1\n",
    "            else:\n",
    "                if corr_mdata==True:\n",
    "                    mdata, report_datum=correct_mdata(strain,md_gisaid[strain])\n",
    "                    report_data+=report_datum\n",
    "                    if mdata.split('\\t')[0]=='XXXX-XX-XX':\n",
    "                        mdata_counts['no date']+=1\n",
    "                    elif int(mdata.split('\\t')[0][:4])<1950 or int(mdata.split('\\t')[0][:4])>int(str(date.today())[:4]):\n",
    "                            print(f\"{strain} has a suspiciously early/late date: {remove_illegal_chars(record.id)}\\t{mdata}\")\n",
    "                    if mdata.split('\\t')[2]=='unknown':\n",
    "                        mdata_counts['not in species']+=1\n",
    "                else:\n",
    "                    mdata='\\t'.join(md_gisaid[strain])\n",
    "        f.write(f'{remove_illegal_chars(record.id)}\\t{mdata}\\n')\n",
    "\n",
    "if missing_species!=[] and report_files==True:\n",
    "    with open(f'{output_dir}/reports/missing_species.tsv', 'w') as f:\n",
    "        for species in sorted(set(missing_species)):\n",
    "            f.write(f'\\t{species}\\n')\n",
    "        print(f'wrote {len(set(missing_species))} missing species to {output_dir}/reports/missing_species.tsv')\n",
    "\n",
    "\n",
    "\n",
    "if report_data != [] and report_files==True:\n",
    "    with open(f'{output_dir}/reports/metadata_correction.txt', 'w') as f:\n",
    "        for report in report_data:\n",
    "            f.write(report+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final filtering of sequences and ORF finding\n",
    "Here we do a bunch of things at once:\n",
    "- Find ORFs for all sequences and filter out sequences without stop codon or with short ORFs\n",
    "- Filter illegal characters from sequence names\n",
    "- Filter out sequences with no medata or no date information (optional)\n",
    "- Filter out 100% identical ORFs and only keep the earliest occurence (optional)\n",
    "- Filter out ORFs with too many inderterminate positions (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initializing counters/lists\n",
    "i = 0\n",
    "orfgood = [] # these are ORFs with a start and a stop\n",
    "orfbad = [] # these are ORFS with a start and a stop but smaller than 90% of the average ORF or\n",
    "            # or ORFs without stop codon\n",
    "mdata_counts['overrides']=0\n",
    "orf_counters={'nostop':0,\n",
    "              'short':0,\n",
    "              'no_mdata':0,\n",
    "              'too_many_ind':0,\n",
    "              'dupes':0,\n",
    "              'kept':0}\n",
    "\n",
    "# read in sequences\n",
    "in_seqs=SeqIO.index(f'{output_dir}/temp/{name}_id_filtered.fasta', \"fasta\")\n",
    "\n",
    "# read in metadata\n",
    "mdata={}\n",
    "with open(f'{output_dir}/metadata/mdata.tsv', 'r') as f:\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        l=line.strip().split('\\t')\n",
    "        mdata[l[0]]=l[1:]\n",
    "\n",
    "\n",
    "for record in SeqIO.parse(f'{output_dir}/temp/{name}_id_filtered.fasta', 'fasta'): # iterating through sequences\n",
    "    if purge_nomdata == True and remove_illegal_chars(record.id) not in mdata.keys():\n",
    "    # no metadata\n",
    "        orfbad.append((record.id, 'no_mdata', record.seq))\n",
    "        orf_counters['no_mdata']+=1\n",
    "    elif purge_nomdata == True and mdata[remove_illegal_chars(record.id)][0]== 'XXXX-XX-XX':\n",
    "    # no date\n",
    "        orfbad.append((record.id, 'no_date', record.seq))\n",
    "        orf_counters['no_mdata']+=1\n",
    "    else:\n",
    "    # everything ok, proceed to find ORFs\n",
    "        seq = record.seq[record.seq.upper().find(\"ATG\"):] # trim 5' end by finding first ATG and slicing \n",
    "        \n",
    "        while len(seq) % 3 != 0: # making seq length a multiple of 3 so biopython doesn't yell at me (Ns will get cut off in next step)\n",
    "            seq += \"N\"\n",
    "        \n",
    "        seq = seq[:len(seq.translate(to_stop=True))*3+3] # trim 3' end by slicing everything after the stop codon\n",
    "\n",
    "        # sometimes the first ATG is not the right one, this part also tests the second ATG\n",
    "        # if the orf from the first was too short\n",
    "        if len(seq) < min_orf_len: \n",
    "            seq2 = record.seq[record.seq[record.seq.upper().find(\"ATG\")+3:].upper().find('ATG')+record.seq.upper().find(\"ATG\")+3:]\n",
    "            while len(seq2) % 3 !=0 :\n",
    "                seq2 += \"N\"\n",
    "            seq2 = seq2[:len(seq2.translate(to_stop=True))*3+3]\n",
    "            # if this ORF is longer, replace the previous small one by this one\n",
    "            if len(seq2) > len(seq):\n",
    "                seq = seq2\n",
    "\n",
    "        if purge_nostop==True and seq.translate()[-1:] != \"*\": # check for stop codon at the end of ORF\n",
    "            orfbad.append((record.id,'nostop', seq))\n",
    "            orf_counters['nostop']+=1\n",
    "        else: # either we're not checking or there is a stop codon\n",
    "            orfgood.append((record.id,seq))\n",
    "\n",
    "## sort out short ORFs (90% cut-off)\n",
    "# 90% is arbitrary, adjust as desired\n",
    "orf_cutoff_nts = sum([len(orf[1]) for orf in orfgood])/len(orfgood)*orf_cutoff\n",
    "\n",
    "# turn orfgood inside out to group ids from identical sequences:\n",
    "orfids=defaultdict(list)\n",
    "for orf in orfgood:\n",
    "    orfids[orf[1].upper()].append(orf[0])\n",
    "\n",
    "replaced=[]\n",
    "with open(f'{output_dir}/temp/{name}_unique_orfs.fasta', \"w\") as f:\n",
    "    for seq in orfids.keys(): # iterate through unique sequences\n",
    "        if inderterminate_ratio(seq.upper())<max_indeterminate:\n",
    "        # check indeterminate character ratio\n",
    "            # check for length, if bad set aside\n",
    "            if len(seq) < orf_cutoff_nts:\n",
    "            # check for length, if bad set aside\n",
    "                for seqid in orfids[seq]:\n",
    "                    orfbad.append((seqid, 'short', seq))\n",
    "                    orf_counters['short']+=1\n",
    "            else:\n",
    "            # length is ok\n",
    "                if purge_unique == True:\n",
    "                    if len(orfids[seq])>1:\n",
    "                    # not unique sequence, figure out which to keep\n",
    "                        v=[remove_illegal_chars(seqid) for seqid in orfids[seq]]\n",
    "                        # find the sequence with earliest date\n",
    "                        # for incomplete dates, first day of first month is taken\n",
    "                        earliest_id =v[[datetime.fromisoformat('-01'.join(mdata[seqid][0].split('-XX'))) for seqid in v].index(min([datetime.fromisoformat('-01'.join(mdata[seqid][0].split('-XX'))) for seqid in v]))]\n",
    "                        v.remove(earliest_id)\n",
    "                        f.write(f\">{remove_illegal_chars(earliest_id)}\\n{seq}\\n\")\n",
    "                        replaced.append(f\"{earliest_id}\\t{','.join(v) if len(v)>1 else v[0]}\\n\")\n",
    "                        orf_counters['kept']+=1\n",
    "                        orf_counters['dupes']+=len(v)\n",
    "                    else:\n",
    "                        f.write(f'>{remove_illegal_chars(orfids[seq][0])}\\n{seq}\\n')\n",
    "                        orf_counters['kept']+=1\n",
    "\n",
    "                else:\n",
    "                # we don't care about uniques\n",
    "                    for seqid in orfids[seq]:\n",
    "                        f.write(f\">{remove_illegal_chars(seqid)}\\n{seq}\\n\")\n",
    "        else:\n",
    "        # too many indeterminate characters\n",
    "            orfbad.append((orf[0], f'too_many_ind-{inderterminate_ratio(orf[1].upper())}', orf[1]))\n",
    "            orf_counters['too_many_ind']+=1\n",
    "\n",
    "\n",
    "# reporting on data\n",
    "print(f'looked for orfs in {sum(orf_counters.values())} sequences:')\n",
    "print(f\"\\t{sum(orf_counters.values())-orf_counters['kept']:>6} ({(sum(orf_counters.values())-orf_counters['kept'])/sum(orf_counters.values())*100:5.2f}%) sequences discarded\")\n",
    "if purge_nomdata==True:\n",
    "    print(f\"\\t\\t{orf_counters['no_mdata']:>6} ({orf_counters['no_mdata']/sum(orf_counters.values())*100:5.2f}%) had no associated metadata or date\")\n",
    "print(f\"\\t\\t{orf_counters['short']:>6} ({orf_counters['short']/sum(orf_counters.values())*100:5.2f}%) were shorter than {orf_cutoff*100:.0f}% of the average ({orf_cutoff_nts:.0f})\")\n",
    "if purge_nostop==True:\n",
    "    print(f\"\\t\\t{orf_counters['nostop']:>6} ({orf_counters['nostop']/sum(orf_counters.values())*100:5.2f}%) had no stop codon\")\n",
    "if max_indeterminate!=1:\n",
    "    print(f\"\\t\\t{orf_counters['too_many_ind']:>6} ({orf_counters['too_many_ind']/sum(orf_counters.values())*100:5.2f}%) had over {max_indeterminate*100:.0f}% indeterminate posiions\")\n",
    "if purge_unique==True:\n",
    "    print(f\"\\t\\t{orf_counters['dupes']:>6} ({orf_counters['dupes']/sum(orf_counters.values())*100:5.2f}%) were not unique\")\n",
    "print(f\"\\t{orf_counters['kept']:>6} ({(orf_counters['kept'])/sum(orf_counters.values())*100:5.2f}%) sequences kept and written to {output_dir}/{name}_unique_orfs.fasta\")\n",
    "\n",
    "if report_files==True:\n",
    "    # sift through all the stuff we exluded\n",
    "    orfbad_output=[]\n",
    "    with open(f'{output_dir}/reports/bad_orfs.tsv', 'w') as f:\n",
    "        for orf in orfbad:\n",
    "            if orf[1] in ('nostop', 'short'):\n",
    "                orfbad_output.append(orf)\n",
    "                detail=len(orf[2])\n",
    "            elif orf[1]=='too_many_ind':\n",
    "                detail=f'{inderterminate_ratio(orf[2]):.2f}'\n",
    "            else:\n",
    "                detail=''\n",
    "            f.write(f'{orf[0]}\\t{orf[1]}\\t{detail}\\n')\n",
    "    \n",
    "    if orfbad_output!=[] and write_badorf==True:\n",
    "        with open(f'{output_dir}/temp/{name}_orfs_bad.fasta', \"w\") as f2:\n",
    "            for item in orfbad_output:\n",
    "                f2.write(f'>{item[0]}_{item[1]}\\n{item[2]}\\n')\n",
    "                f2.write(f'>{item[0]}\\n{str(in_seqs[item[0]].seq)}\\n')\n",
    "\n",
    "    # report on replacements\n",
    "    with open(f'{output_dir}/reports/replaced.tsv', 'w') as f:\n",
    "        for item in replaced:\n",
    "            f.write(item)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blacklist filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data=[]\n",
    "# read in blacklist\n",
    "with open(f'{output_dir}/metadata/blacklist.txt', 'r') as f:\n",
    "    f.readline() # skip header\n",
    "    blacklist=[remove_illegal_chars(line.strip().split('\\t')[0]) for line in f]\n",
    "\n",
    "with open(f'{output_dir}/data/{name}.fasta' ,'w') as f:\n",
    "    for record in SeqIO.parse(f'{output_dir}/temp/{name}_unique_orfs.fasta' ,'fasta'):\n",
    "        if record.id not in blacklist:\n",
    "            f.write(f'>{record.id}\\n{record.seq}\\n')\n",
    "        else:\n",
    "            report_data.append(f'sequence {record.id} was in blacklist and has been removed')\n",
    "            blacklist.remove(record.id)\n",
    "\n",
    "blacklisted=len(report_data)\n",
    "print(f'removed {blacklisted} blacklisted sequences, {len(blacklist)} blacklisted IDs were not found in the data')\n",
    "\n",
    "if report_files==True:\n",
    "    with open(f'{output_dir}/reports/blacklisted.txt', 'w') as f:\n",
    "        for report in report_data:\n",
    "            f.write(report+'\\n')\n",
    "        for seqid in blacklist:\n",
    "            f.write(f'did not find {seqid} in data\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should now be ready for alignment and tree building.\n",
    "\n",
    "Of course, there's always more that can be done, starting with getting some stats on the whole process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Some info and stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to output a file that sums up what was done to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_dir}/parameters.txt', 'w') as f:\n",
    "    f.write(f'dataset {name} was cleaned up using pépinière v{version} on {date.today()}\\n')\n",
    "    if report_files==True:\n",
    "        f.write(f\"reports were generated and written to {output_dir}/reports\\n\")\n",
    "    else:\n",
    "        f.write('no reports were generated\\n')\n",
    "\n",
    "\n",
    "    f.write(f'\\n{\"_\"*60}\\n')\n",
    "    f.write('INPUT SEQUENCES\\n')\n",
    "    f.write(f'input files were: {seq_files[0][0]} ({seq_files[0][1]} format)\\n')\n",
    "    for seq_file in seq_files[1:]:\n",
    "        f.write(f\"{' '*18}{seq_file[0]} ({seq_file[1]} format)\\n\")\n",
    "    f.write(f'\\nfound and removed {seq_counts[\"total\"]-seq_counts[\"dedup\"]} duplicate sequences based on accession/EPI numbers ({(seq_counts[\"total\"]-seq_counts[\"dedup\"])/seq_counts[\"total\"]*100:.2f}%)\\n')\n",
    "    f.write(f\"final set contains {db_counts['gisaid']+db_counts['bvbrc']} sequences,\"+\n",
    "        f\" of which {db_counts['gisaid']} from GISAID ({db_counts['gisaid']/(db_counts['gisaid']+db_counts['bvbrc'])*100:.2f}%)\"+\n",
    "        f\" and {db_counts['bvbrc']} from BVBRC ({db_counts['bvbrc']/(db_counts['bvbrc']+db_counts['gisaid'])*100:.2f}%),\"+\n",
    "        f\" {db_counts['both']} of which were also in GISAID ({db_counts['both']/db_counts['bvbrc']*100:.2f}%)\\n\")\n",
    "    \n",
    "\n",
    "    f.write(f'\\n{\"_\"*60}\\n')\n",
    "    f.write('INPUT METADATA\\n')\n",
    "    f.write(f'\\nmetadata files were: {metadata_files[0][0]} ({metadata_files[0][1]} format)\\n')\n",
    "    for metadata_file in metadata_files[1:]:\n",
    "        f.write(f\"{' '*21}{metadata_file[0]} ({metadata_file[1]} format)\\n\")\n",
    "    f.write(f\"\\nmetadata was{' not' if corr_mdata==False else ''} corrected and consolidated to {output_dir}/metadata/mdata.tsv\\n\")\n",
    "    f.write(f\"species list used was {species_list} containing {len(species_dict)} entries but missing {len(set(missing_species))} entries\\n\")\n",
    "    f.write(f\"override file contained {mdata_counts['overrides']} entries, {mdata_counts['overriden']} entries were overriden\")\n",
    "    f.write(f\"\\nof {mdata_counts['total']} processed metadata entries:\\n\")\n",
    "    f.write(f\"\\t{mdata_counts['no data']:>5} had no metadata info{' '*6}({mdata_counts['no data']/mdata_counts['total']*100:5.2f}%)\\n\")\n",
    "    f.write(f\"\\t{mdata_counts['no date']:>5} had no date info{' '*10}({mdata_counts['no date']/mdata_counts['total']*100:5.2f}%)\\n\")\n",
    "    f.write(f\"\\t{mdata_counts['not in species']:>5} had no species group info ({mdata_counts['not in species']/mdata_counts['total']*100:5.2f}%)\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    f.write(f'\\n{\"_\"*60}\\n')\n",
    "    f.write('ORF TRIMMING\\n')\n",
    "    f.write(f'looked for orfs in {sum(orf_counters.values())} sequences:\\n')\n",
    "    f.write(f\"\\t{sum(orf_counters.values())-orf_counters['kept']:>6} ({(sum(orf_counters.values())-orf_counters['kept'])/sum(orf_counters.values())*100:5.2f}%) sequences discarded\\n\")\n",
    "    f.write(f\"\\t\\t{orf_counters['short']:>6} ({orf_counters['short']/sum(orf_counters.values())*100:5.2f}%) were shorter than {orf_cutoff*100:.0f}% of the average ({orf_cutoff_nts:.0f})\\n\")\n",
    "    if purge_nomdata==True:\n",
    "        f.write(f\"\\t\\t{orf_counters['no_mdata']:>6} ({orf_counters['no_mdata']/sum(orf_counters.values())*100:5.2f}%) had no associated metadata or date\\n\")\n",
    "    else:\n",
    "        f.write('\\t\\tsequences without metadata were not removed from the dataset\\n')\n",
    "    if purge_nostop==True:\n",
    "        f.write(f\"\\t\\t{orf_counters['nostop']:>6} ({orf_counters['nostop']/sum(orf_counters.values())*100:5.2f}%) had no stop codon\\n\")\n",
    "    else:\n",
    "        f.write('\\t\\tORFs without stop codon were not removed from the dataset\\n')\n",
    "    if max_indeterminate!=1:\n",
    "        f.write(f\"\\t\\t{orf_counters['too_many_ind']:>6} ({orf_counters['too_many_ind']/sum(orf_counters.values())*100:5.2f}%) had over {max_indeterminate*100:.0f}% indeterminate posiions\\n\")\n",
    "    else:\n",
    "        f.write('\\t\\tORFs with indeterminate positions were not removed from the dataset\\n')\n",
    "    if purge_unique==True:\n",
    "        f.write(f\"\\t\\t{orf_counters['dupes']:>6} ({orf_counters['dupes']/sum(orf_counters.values())*100:5.2f}%) were not unique\\n\")\n",
    "    else:\n",
    "        f.write('\\t\\tduplicate ORFs were not removed from the dataset\\n')\n",
    "    f.write(f\"\\t{orf_counters['kept']:>6} ({(orf_counters['kept'])/sum(orf_counters.values())*100:5.2f}%) sequences kept and written to {output_dir}/{name}_unique_orfs.fasta\\n\")\n",
    "\n",
    "    if len(blacklist) != 0:\n",
    "        f.write(f'\\n blacklist contained {len(blacklist)} sequences, {blacklisted} of which were found and removed from the final data')\n",
    "\n",
    "    f.write(f'\\nfinal dataset contains {orf_counters[\"kept\"]-blacklisted} of {seq_counts[\"total\"]} sequences ({orf_counters[\"kept\"]/seq_counts[\"total\"]*100:.2f}%)')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks in more detail at the sequence replacement part of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_ranges=Counter()\n",
    "regions_counts=Counter()\n",
    "species_counts=Counter()\n",
    "\n",
    "if report_files==True:\n",
    "    with open(f'{output_dir}/reports/replaced.tsv', 'r') as f:\n",
    "        groups=[[line.split('\\t')[0]]+line.strip().split('\\t')[1].split(',') for line in f]\n",
    "    with (open(f'{output_dir}/reports/heterogeneous_years.tsv', 'w') as f_years,\n",
    "          open(f'{output_dir}/reports/heterogeneous_region.tsv', 'w') as f_region,\n",
    "          open(f'{output_dir}/reports/heterogeneous_species.tsv', 'w') as f_species):\n",
    "        f_years.write('group\\tyears\\tspecies\\tregion\\n')\n",
    "        f_region.write('group\\tyears\\tspecies\\tregion\\n')\n",
    "        f_species.write('group\\tyears\\tspecies\\tregion\\n')\n",
    "        for group in groups:\n",
    "            years=[int(mdata[seqid][0].split('-')[0]) for seqid in group]\n",
    "            year_range=max(years)-min(years)\n",
    "            year_ranges.update((year_range,))\n",
    "            regions=[mdata[seqid][3] for seqid in group if mdata[seqid][3]!= 'unknown']\n",
    "            regions_counts.update((max(0,len(set(regions))-1),))\n",
    "            species=[mdata[seqid][2] for seqid in group if mdata[seqid][2] not in  ('unknown', 'not determined')]\n",
    "            species_counts.update((max(0,len(set(species))-1),))\n",
    "\n",
    "            # write to file,\n",
    "            if year_range > year_gap_threshold:\n",
    "                f_years.write(f\"{','.join(group)}\\t{','.join(map(str,years))}\\t{','.join(regions)}\\t{','.join(species)}\\n\")\n",
    "            if len(set(regions))>1:\n",
    "                f_region.write(f\"{','.join(group)}\\t{','.join(map(str,years))}\\t{','.join(regions)}\\t{','.join(species)}\\n\")\n",
    "            if len(set(species))>1:\n",
    "                f_species.write(f\"{','.join(group)}\\t{','.join(map(str,years))}\\t{','.join(regions)}\\t{','.join(species)}\\n\")\n",
    "    if plotting == True:\n",
    "        data=pd.DataFrame([[k, v, 'year'] for k, v in year_ranges.items()]+[[k, v, 'region'] for k, v in regions_counts.items()]+[[k, v, 'species'] for k, v in species_counts.items()],\n",
    "                        columns=['differences', 'count', 'metric'])\n",
    "        fig=px.bar(data, x='differences',y='count',facet_col='metric')\n",
    "        fig.update_xaxes(matches=None)\n",
    "        fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Treework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning\n",
    "Try to align our dataset using MAFFT. MAFFT needs to be installed and in the path of your computer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toggle_align==True:\n",
    "    if alignment_run==True:\n",
    "        rerun=input('alignment has already been run, should i rerun it? y/n')\n",
    "    else:\n",
    "        rerun='y'\n",
    "    if rerun == 'y':\n",
    "        cmd = f\"mafft --reorder --anysymbol --thread 8 --nomemsave {quote(f'{output_dir}/data/{name}.fasta')} 1> {quote(f'{output_dir}/data/{name}_mafft.fasta')}\" \n",
    "        !{cmd}\n",
    "        alignment_run=True\n",
    "    else:\n",
    "        print(f'answer was {rerun}, alignment will not be rerun')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree making\n",
    "Calling fasttree on the alignment. Fasttree needs to be in the path of your computer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toggle_tree == True:\n",
    "    if tree_run == True:\n",
    "        rerun=input('tree was already run, should i rerun it ? y/n')\n",
    "    else:\n",
    "        rerun='y'\n",
    "    if rerun == 'y':\n",
    "        cmd= f\"fasttree -nt -gtr -gamma {quote(f'{output_dir}/data/{name}_mafft.fasta')} > {quote(f'{output_dir}/tree/{name}_mafft_fast.tree')}\"\n",
    "        !{cmd}\n",
    "        tree_run=True\n",
    "    else:\n",
    "        'tree has already been made and will not be rerun'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treetime\n",
    "Calling treetime on the alignment, tree and metadata. Generate a newick format tree that can be uploaded to iTOL (which does not like the default nexus output tree). Treetime needs to be in the path of your computer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toggle_treetime==True:\n",
    "    # first we need to actually incorporate the metadata override into the metadata file\n",
    "    if treetime_run==True:\n",
    "        rerun=input('treetime was already called, should i rerun it? y/n')\n",
    "    else:\n",
    "        rerun='y'\n",
    "    if rerun=='y':\n",
    "        cmd = f\"treetime --aln {quote(f'{output_dir}/data/{name}_mafft.fasta')} --tree {quote(f'{output_dir}/tree/{name}_mafft_fast.tree')} --dates {quote(f'{output_dir}/metadata/mdata.tsv')} --outdir {quote(f'{output_dir}/tree/')}\"\n",
    "        !{cmd}\n",
    "        treetime_run=True\n",
    "        with open(f'{output_dir}/tree/timetree.nexus', 'r') as fin, open(f'{output_dir}/tree/fasttimetree.nwk', 'w') as fout:\n",
    "            for line in fin:\n",
    "                if line.startswith(' Tree tree1='):\n",
    "                    fout .write(line.split('tree1=')[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotations\n",
    "Making annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define legend categories and colors\n",
    "colordict_country = {'North_America':'#ff8282',\n",
    "                'South_America':'#ffb882',\n",
    "                'Europe':'#82a1ff',\n",
    "                'Africa':'#fff082',\n",
    "                'Asia':'#84ff82',\n",
    "                'Oceania':'#ff82fb',\n",
    "                'Antarctica':'#e8ffff'\n",
    "                }\n",
    "\n",
    "colordict_species = {'terrestrial poultry':'#EE7733',\n",
    "            'Anseri/Charadriiformes':'#33BBEE',\n",
    "            'other birds':'#228833',\n",
    "            'other':'#BBBBBB',\n",
    "            'human':'#831491',\n",
    "            'mammalian':'#d881e3'\n",
    "            }\n",
    "\n",
    "# reread in the species file just in case\n",
    "with open(species_list, 'r') as f:\n",
    "    species_dict={line.strip().split('\\t')[1]:line.split('\\t')[0] for line in f}\n",
    "\n",
    "# reread in mdata just in case including override\n",
    "# read in metadata\n",
    "mdata={}\n",
    "with open(f'{output_dir}/metadata/mdata.tsv', 'r') as f:\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        l=line.strip().split('\\t')\n",
    "        mdata[l[0]]=l[1:]\n",
    "# read in overrides\n",
    "with open(f'{output_dir}/metadata/override.tsv', 'r') as f:\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        l=line.strip().split('\\t')\n",
    "        mdata[l[0]]=l[1:]\n",
    "\n",
    "# go through mdata and put into annotation files\n",
    "with (open(f'{output_dir}/annotations/tree_annotation_strip_geo.txt','w') as f_geo, \n",
    "        open(f'{output_dir}/annotations/tree_annotation_strip_species.txt','w') as f_species):\n",
    "    \n",
    "    # write headers and legends for annotation files\n",
    "    f_geo.write('DATASET_COLORSTRIP\\nSEPARATOR COMMA\\nDATASET_LABEL,geo\\nCOLOR,#fdbf6f\\nCOLOR_BRANCHES,0\\nLEGEND_TITLE,continent\\n'+\n",
    "                f'LEGEND_SHAPES,{\",\".join([\"1\" for k in colordict_country.keys()])}\\nLEGEND_COLORS,{\",\".join([colordict_country[k] for k in sorted(colordict_country.keys())])}\\nLEGEND_LABELS,{\",\".join([k for k in sorted(colordict_country.keys())])}\\n'+\n",
    "                'MARGIN,5\\nSHOW_STRIP_LABELS,0\\nDATA\\n')\n",
    "    f_species.write('DATASET_COLORSTRIP\\nSEPARATOR COMMA\\nDATASET_LABEL,species\\nCOLOR,#fdbf6f\\nCOLOR_BRANCHES,0\\nLEGEND_TITLE,species category\\n'+\n",
    "                    f'LEGEND_SHAPES,{\",\".join([\"1\" for k in colordict_species.keys()])}\\nLEGEND_COLORS,{\",\".join([colordict_species[k] for k in sorted(colordict_species.keys())])}\\nLEGEND_LABELS,{\",\".join([k for k in sorted(colordict_species.keys())])}\\n'+\n",
    "                    'MARGIN,5\\nSHOW_STRIP_LABELS,0\\nDATA\\n')\n",
    "    \n",
    "    # write annotation data\n",
    "    for record in SeqIO.parse(f'{output_dir}/data/{name}.fasta', 'fasta'):\n",
    "        \n",
    "        if mdata[record.id][2] != 'unknown':\n",
    "            f_species.write(f\"{record.id},{colordict_species[mdata[record.id][2]]},{mdata[record.id][2]}\\n\")\n",
    "        \n",
    "        if mdata[record.id][3] !='unknown':\n",
    "            f_geo.write(f\"{record.id},{colordict_country[mdata[record.id][3]]},{mdata[record.id][3]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "417915ee3369acde11c6b3933ba362a9526ec24c3d87f73bff43ddb0d5881961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
